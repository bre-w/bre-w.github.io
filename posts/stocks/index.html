<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" href="../../css/main.css">
  <title>Stock Prediction with Deep Q Learning</title>
  <div class="header">
				<nav>
				<ul>
					<li><a href="/">Home</a></li>
					<li><a href="/about">About</a></li>
					<li><a href="/cv">CV</a></li>
					<li><a href="/blog">Posts</a></li>
				</ul>
				</nav>
				<h1>{{ page.title}}</h1>
			</div>
</head>
<body>

<hr>
<h2>Problem Setup</h2>
<hr>

Our goal is to create a model which will recommend what action to take today given the previous n days of data as input. More specifically, the input will be an n-length array of normalized daily adjusted close prices for a single stock. For this basic model we will have three possible actions:
<ol>
  <li>Sell all held stocks</li>
  <li>Buy with all available cash</li>
  <li>Do nothing</li>
</ol>

We will train a neural network from scratch using TensorFlow and the reinforcement learning technique known as Q learning.

<h2>Q Learning</h2>
<hr>

Q learning is simply a mapping of a state, action pair (s, a) to a scalar value known as the q value. This value is indicative of the expected long term reward. Given a state s, the action a which has the largest expected cumulative reward will have the largest q value. Thus, if we are able to map all state, action pairs to a q value we can act optimally by simply selecting the action with the largest q value for the given state we are in.

<h3>Learning the Mappings</h3>

To learn the mappings we explore the "world". This essentially involves moving around from state to state based on actions chosen and obtaining rewards during these state transitions. The goal is to get a q value as a function of the state action pair, Q(s, a), for all possible state action pairs. Note that we first initialize q values for each s, a pair typically to random small values or zeros. The procedure is as follows:
<ol>
  <li>Begin in some state, s (in our case we would start at some day, and the state would be the stock prices of the previous n days)</li>
  <li>Take an action, a. This is often done using an e-greedy policy. In this policy we take a random action with probability e, and we take the assumed optimal action with probability `1 - e`</li>
  <li>Based on the action taken, move into the next state, s'</li>
  <li>Get a reward, r, based on the transition from s to s'</li>
  <li>Obtain the highest possible q value, q', for this new state s'. That is, scan the q values over all possible actions given s', and select the highest q value</li>
  <li>Set the Q(s, a) mapping = r + gamma * q' where gamma is a reward discount factor</li>
  </ol>
  
Step 6 is where the neural network comes in handy. However, if the state-action space is small enough we don't need a function approximator (e.g. neural network) and a table will suffice. For example, instead of using a n-length vector as our state, let's use a scalar. Specifically, let's use a common technical indicator such as [Bollinger Bands](https://en.wikipedia.org/wiki/Bollinger_Bands). We could discretize this continuous value into buckets, let's say five for example. Then the total number of unique state, action spaces is `5 * 3 = 15`, a very small space which we are almost certainly to explore multiple times. In fact, we certainly don't need to limit ourselves to a single technical indicator. Let's say we use four various indicators, each discretized into five buckets. Then the total number of unique state, action spaces is `5`<sup>`4`</sup>` * 3 = 1875`, still a very reasonable number. If, however, we wanted to use the previous 30 days stock prices (n=30) and we are discretizing each price into five buckets, we now have `5`<sup>`30`</sup>` * 3 = 3e21` unique state action pairs. This creates two main problems:
<ol>
  <li>This is far too big a space to exhastively explore</li>
  <li>This requires far too much memory to store such a table</li>
</ol>
<h3>Function Approximation</h3>

Instead of a table mapping state, action pairs to a values we will use a function approximator. Specifically, we will use a neural networks (nn) as are function approximator due to its ability to approximate any function given sufficient architecture and relatively smooth boundaries. Using this approximator solves the two problems above. Assuming nearby states have similar q values we don't need to explore every possible state, action to get reasonable estimates for q. Also, we don't need to store q values for every state, action pair, only the parameters of the network that will let us cacluate the q estimate when needed. When implementing the nn, we will have it take a state as the input and output a q value for each action. We could combine an action with the state and use that as the input to the nn with a single q value output, but the former method requires less passes through the network and is more efficient.

<h2>Neural Network Architecture</h2>
<hr>

As mentioned previously, our input to the network will be an array of the previous n days of stock prices. Although n will be a paramter we can change, for this example we'll use 30, so essentially a one month look back.

  <footer>
	    		<ul>
	        		<li><a href="mailto:brendanthewilliams@gmail.com">email</a></li>
	        		<li><a href="https://github.com/bre-w">github.com/bre-w</a></li>
				</ul>
			</footer>
</body>
</html>

---
layout: post
title: "Stock Prediction with Deep Q Learning"
date: 2020-07-06
---
## Problem Setup
---

Our goal is to create a model which will recommend what action to take today given the previous n days of data as input. More specifically, the input will be an n-length array of normalized daily adjusted close prices for a single stock. For this basic model we will have three possible actions:
1. Sell all held stocks
2. Buy with all available cash
3. Do nothing

We will train a neural network from scratch using TensorFlow and the reinforcement learning technique known as Q learning.

## Q Learning
---

Q learning is simply a mapping of a state, action pair (s, a) to a scalar value known as the q value. This value is indicative of the expected long term reward. Given a state s, the action a which has the largest expected cumulative reward will have the largest q value. Thus, if we are able to map all state, action pairs to a q value we can act optimally by simply selecting the action with the largest q value for the given state we are in.

### Learning the Mappings

To learn the mappings we explore the "world". This essentially involves moving around from state to state based on actions chosen and obtaining rewards during these state transitions. The goal is to get a q value as a function of the state action pair, Q(s, a), for all possible state action pairs. Note that we first initialize q values for each s, a pair typically to random small values or zeros. The procedure is as follows:

1. Begin in some state, s (in our case we would start at some day, and the state would be the stock prices of the previous n days)
2. Take an action, a. This is often done using an e-greedy policy. In this policy we take a random action with probability e, and we take the assumed optimal action with probability 1 - e
3. Based on the action taken, move into the next state, s'
4. Get a reward, r, based on the transition from s to s'
5. Obtain the highest possible q value, q', for this new state s'. That is, scan the q values over all possible actions given s', and select the highest q value
6. Set the Q(s, a) mapping = r + gamma * q' where gamma is a reward discount factor

Step 6 is where the neural network comes in handy. However, if the state-action space is small enough we don't need a function approximator (e.g. neural network) and a table will suffice. For example, instead of using a n-length vector as our state, let's use a scalar. Specifically, let's use a common technical indicator such as [Bollinger Bands](https://en.wikipedia.org/wiki/Bollinger_Bands). We could discretize this continuous value into buckets, let's say five for example. Then the total number of unique state, action spaces is 5 * 3 = 15, a very small space which we are almost certainly to explore multiple times. In fact, we certainly don't need to limit ourselves to a single technical indicator. Let's say we use four various indicators, each discretized into five buckets. Then the total number of unique state, action spaces is '5^4*3 = 1875', still a very reasonable number. If, however, we wanted to use the previous 30 days stock prices (n=30) and we are discretizing each price into five buckets, we now have 5^30*3 = 3e21 unique state action pairs. This is not only far too many to be able to explore but also too many entries to store in our table. I

